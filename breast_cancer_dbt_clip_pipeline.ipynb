{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f866cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# Breast Cancer DBT — Leak-Safe CLIP + MLP Pipeline\n",
    "# -----------------------------------------------\n",
    "# • Reproducible: auto-installs deps and downloads data from Kaggle\n",
    "# • Leak-safe: split BEFORE any augmentation/resampling\n",
    "# • Imbalance-aware: SMOTETomek + Focal Loss + ENS class weights\n",
    "# • Exports: JPG figures + Word (.docx) files (caption + description)\n",
    "# ===============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a77dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- [0] Robust installs (works in .py or notebook) ----------\n",
    "import sys, subprocess, importlib, warnings, os\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "def _pip_install(args):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\"] + args)\n",
    "\n",
    "def _ensure(pkg, pypi_name=None):\n",
    "    try:\n",
    "        return importlib.import_module(pkg)\n",
    "    except Exception:\n",
    "        _pip_install([pypi_name or pkg])\n",
    "        return importlib.import_module(pkg)\n",
    "\n",
    "# Core scientific stack\n",
    "np = _ensure(\"numpy\")\n",
    "pd = _ensure(\"pandas\")\n",
    "matplotlib = _ensure(\"matplotlib\"); plt = importlib.import_module(\"matplotlib.pyplot\")\n",
    "sns = _ensure(\"seaborn\")\n",
    "PIL = _ensure(\"PIL\", \"Pillow\"); from PIL import Image\n",
    "\n",
    "# ML / metrics\n",
    "sklearn = _ensure(\"sklearn\", \"scikit-learn\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score,\n",
    "                             precision_score, recall_score, roc_auc_score, roc_curve, auc,\n",
    "                             precision_recall_curve, average_precision_score)\n",
    "\n",
    "# Imbalance + aug\n",
    "imblearn = _ensure(\"imblearn\", \"imbalanced-learn\"); from imblearn.combine import SMOTETomek\n",
    "alb = _ensure(\"albumentations\"); import albumentations as A\n",
    "_ensure(\"cv2\", \"opencv-python-headless\")\n",
    "\n",
    "# Word export\n",
    "docx = _ensure(\"docx\", \"python-docx\"); from docx import Document\n",
    "from docx.shared import Inches, Pt\n",
    "\n",
    "# Torch (try default; if missing, fall back to CPU index)\n",
    "try:\n",
    "    import torch\n",
    "except Exception:\n",
    "    try:\n",
    "        _pip_install([\"torch\", \"torchvision\", \"torchaudio\"])\n",
    "    except Exception:\n",
    "        _pip_install([\"torch\", \"torchvision\", \"torchaudio\", \"--index-url\",\n",
    "                      \"https://download.pytorch.org/whl/cpu\"])\n",
    "    import torch\n",
    "\n",
    "# CLIP without Git\n",
    "try:\n",
    "    import clip\n",
    "except Exception:\n",
    "    _pip_install([\"clip-anytorch\"])\n",
    "    import clip\n",
    "\n",
    "# Kaggle downloader\n",
    "kgh = _ensure(\"kagglehub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- [1] Configuration ----------\n",
    "# # Repro paths (project-relative)\n",
    "ROOT = Path(__file__).resolve().parent if \"__file__\" in globals() else Path(\".\").resolve()\n",
    "DATA_ROOT = ROOT / \"data\"                # downloaded dataset will live here (cache symlink)\n",
    "RESULTS_DIR = ROOT / \"results\"           # all outputs saved here\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Classes as shipped in dataset folders\n",
    "CLASSES = ['Benign', 'Actionable', 'Cancer', 'Normal']\n",
    "class_to_idx = {c: i for i, c in enumerate(CLASSES)}\n",
    "idx_to_class = {i: c for c, i in class_to_idx.items()}\n",
    "IMG_SUFFIXES = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "# # Reproducibility + device\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "import random; random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6cec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- [2] Download dataset from Kaggle (reproducible) ----------\n",
    "# # This mirrors the path you shared previously but downloads automatically.\n",
    "# # If the dataset is already cached, kagglehub returns the existing path.\n",
    "print(\"[INFO] Downloading dataset via kagglehub …\")\n",
    "dataset_cache_dir = kgh.dataset_download(\"gabrielcarvalho11/breast-cancer-screening-dbt\", version=1)\n",
    "DATASET_DIR = Path(dataset_cache_dir) / \"Breast-Cancer-Screening-DBT\"\n",
    "DATA_ROOT.mkdir(exist_ok=True, parents=True)  # optional local mount\n",
    "print(f\"[INFO] Dataset root: {DATASET_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ---------- [3] Export helpers (JPG + Word only) ----------\n",
    "def save_df_to_docx(df: \"pd.DataFrame\", path: Path, caption: str, description: str):\n",
    "    \"\"\"# Save a table to Word with caption above and description below.\"\"\"\n",
    "    doc = Document()\n",
    "    p_cap = doc.add_paragraph(); run = p_cap.add_run(f\"Table: {caption}\")\n",
    "    run.bold = True; run.font.size = Pt(12)\n",
    "\n",
    "    table = doc.add_table(rows=df.shape[0] + 1, cols=df.shape[1])\n",
    "    table.style = \"Light Grid\"\n",
    "    for j, col in enumerate(df.columns):\n",
    "        table.cell(0, j).text = str(col)\n",
    "    for i in range(df.shape[0]):\n",
    "        for j in range(df.shape[1]):\n",
    "            table.cell(i + 1, j).text = str(df.iloc[i, j])\n",
    "\n",
    "    doc.add_paragraph(\"\")  # spacer\n",
    "    p_desc = doc.add_paragraph(description); p_desc.runs[0].italic = True\n",
    "    doc.save(str(path))\n",
    "\n",
    "def save_figure_jpg_and_docx(fig, jpg_path: Path, docx_path: Path, caption: str, description: str):\n",
    "    \"\"\"# Save a figure as JPG and wrap it in a Word doc (caption + description).\"\"\"\n",
    "    fig.savefig(str(jpg_path), dpi=300, bbox_inches=\"tight\", format=\"jpg\")\n",
    "    plt.close(fig)\n",
    "    doc = Document()\n",
    "    p_cap = doc.add_paragraph(); run = p_cap.add_run(f\"Figure: {caption}\")\n",
    "    run.bold = True; run.font.size = Pt(12)\n",
    "    doc.add_picture(str(jpg_path), width=Inches(6.5))\n",
    "    doc.add_paragraph(\"\")\n",
    "    p_desc = doc.add_paragraph(description); p_desc.runs[0].italic = True\n",
    "    doc.save(str(docx_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d30a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- [4] Dataset listing + sanity checks ----------\n",
    "def list_images_by_class(root: Path):\n",
    "    \"\"\"# Walk class folders and return image paths, labels, and counts.\"\"\"\n",
    "    paths, labels, counts = [], [], {}\n",
    "    for c in CLASSES:\n",
    "        folder = root / c\n",
    "        imgs = [p for p in folder.glob(\"*\") if p.suffix.lower() in IMG_SUFFIXES]\n",
    "        counts[c] = len(imgs)\n",
    "        for p in imgs:\n",
    "            paths.append(p)\n",
    "            labels.append(class_to_idx[c])\n",
    "    return np.array(paths), np.array(labels), counts\n",
    "\n",
    "def plot_class_hist(counts_dict, title):\n",
    "    \"\"\"# Barplot of class counts.\"\"\"\n",
    "    fig = plt.figure(figsize=(7.2, 5))\n",
    "    names = list(counts_dict.keys()); vals = [counts_dict[k] for k in names]\n",
    "    sns.barplot(x=names, y=vals)\n",
    "    plt.ylabel(\"Number of images\"); plt.xticks(rotation=20); plt.title(title); plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# List & visualize all images\n",
    "all_paths, all_labels, all_counts = list_images_by_class(DATASET_DIR)\n",
    "if len(all_paths) == 0:\n",
    "    raise SystemExit(\"No images found. Check dataset structure under:\\n\" + str(DATASET_DIR))\n",
    "\n",
    "fig = plot_class_hist(all_counts, \"Class distribution (all images)\")\n",
    "save_figure_jpg_and_docx(\n",
    "    fig,\n",
    "    RESULTS_DIR / \"Figure_Class_Distribution_All.jpg\",\n",
    "    RESULTS_DIR / \"Figure_Class_Distribution_All.docx\",\n",
    "    caption=\"Class distribution across all images.\",\n",
    "    description=\"Number of images per class before any splitting or resampling.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaad73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------- [5] Split BEFORE any transforms (leak-safe) ----------\n",
    "train_paths, test_paths, y_train, y_test = train_test_split(\n",
    "    all_paths, all_labels, test_size=0.20, random_state=RANDOM_STATE, stratify=all_labels\n",
    ")\n",
    "train_paths, val_paths, y_train, y_val = train_test_split(\n",
    "    train_paths, y_train, test_size=0.20, random_state=RANDOM_STATE, stratify=y_train\n",
    ")\n",
    "\n",
    "from collections import Counter\n",
    "def counts_from_labels(y):\n",
    "    c = Counter(y); return {idx_to_class[i]: c.get(i, 0) for i in sorted(c)}\n",
    "\n",
    "fig = plot_class_hist(counts_from_labels(y_train), \"Class distribution (train split)\")\n",
    "save_figure_jpg_and_docx(\n",
    "    fig,\n",
    "    RESULTS_DIR / \"Figure_Class_Distribution_Train.jpg\",\n",
    "    RESULTS_DIR / \"Figure_Class_Distribution_Train.docx\",\n",
    "    caption=\"Training split class distribution.\",\n",
    "    description=\"Counts of each class in the training split. All resampling is performed only on this split.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b4fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- [6] CLIP features (augment train only) ----------\n",
    "# # Load CLIP encoder (ViT-B/32)\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model.eval()\n",
    "\n",
    "# # Lightweight augmentations for train set\n",
    "AUG = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.35),\n",
    "    A.RandomBrightnessContrast(p=0.35),\n",
    "    A.ShiftScaleRotate(shift_limit=0.08, scale_limit=0.12, rotate_limit=20, p=0.45),\n",
    "], p=1.0)\n",
    "\n",
    "def load_image_preprocess(path, augment=False):\n",
    "    \"\"\"# Load PIL → optional Albumentations → CLIP preprocess tensor.\"\"\"\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    if augment:\n",
    "        arr = np.array(img)\n",
    "        arr = AUG(image=arr)[\"image\"]\n",
    "        img = Image.fromarray(arr)\n",
    "    return preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_path(path, augment=False):\n",
    "    \"\"\"# Encode one image path to a 512-d CLIP feature.\"\"\"\n",
    "    x = load_image_preprocess(path, augment=augment)\n",
    "    feat = clip_model.encode_image(x)\n",
    "    return feat.squeeze(0).cpu().numpy()\n",
    "\n",
    "# # Choose minority classes from TRAIN only (for targeted aug)\n",
    "tr_counts = counts_from_labels(y_train)\n",
    "minority_classes = [c for c, _ in sorted(tr_counts.items(), key=lambda kv: kv[1])[:2]]\n",
    "\n",
    "def extract_features(paths, y, augment_for_classes=None):\n",
    "    \"\"\"# Encode a list of image paths to CLIP features (train can be augmented).\"\"\"\n",
    "    X, y_out = [], []\n",
    "    for p, lab in zip(paths, y):\n",
    "        cls_name = idx_to_class[int(lab)]\n",
    "        aug = (augment_for_classes is not None) and (cls_name in augment_for_classes)\n",
    "        X.append(encode_path(p, augment=aug))\n",
    "        y_out.append(lab)\n",
    "    return np.vstack(X), np.array(y_out)\n",
    "\n",
    "print(\"[INFO] Encoding CLIP features (train/val/test)…\")\n",
    "X_train, y_train = extract_features(train_paths, y_train, augment_for_classes=minority_classes)\n",
    "X_val,   y_val   = extract_features(val_paths,   y_val,   augment_for_classes=None)\n",
    "X_test,  y_test  = extract_features(test_paths,  y_test,  augment_for_classes=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f194ee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ---------- [7] Handle imbalance on TRAIN only ----------\n",
    "print(\"[INFO] Applying SMOTETomek on training features …\")\n",
    "resampler = SMOTETomek(random_state=RANDOM_STATE)\n",
    "X_train_res, y_train_res = resampler.fit_resample(X_train, y_train)\n",
    "\n",
    "fig = plot_class_hist(counts_from_labels(y_train), \"Training distribution (original)\")\n",
    "save_figure_jpg_and_docx(\n",
    "    fig, RESULTS_DIR / \"Figure_Train_Distribution_Original.jpg\", RESULTS_DIR / \"Figure_Train_Distribution_Original.docx\",\n",
    "    caption=\"Original training distribution.\",\n",
    "    description=\"Class counts in the training set before resampling.\"\n",
    ")\n",
    "fig = plot_class_hist(counts_from_labels(y_train_res), \"Training distribution (SMOTETomek)\")\n",
    "save_figure_jpg_and_docx(\n",
    "    fig, RESULTS_DIR / \"Figure_Train_Distribution_Resampled.jpg\", RESULTS_DIR / \"Figure_Train_Distribution_Resampled.docx\",\n",
    "    caption=\"Training distribution after SMOTETomek.\",\n",
    "    description=\"Class counts after applying SMOTETomek to mitigate class imbalance.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7e5151",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ---------- [8] Simple MLP classifier with Focal Loss ----------\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MammogramDS(Dataset):\n",
    "    \"\"\"# Tensor dataset for CLIP features.\"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"# 2-layer MLP with BN + Dropout.\"\"\"\n",
    "    def __init__(self, in_dim=512, hidden=256, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden); self.bn1 = nn.BatchNorm1d(hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden//2); self.bn2 = nn.BatchNorm1d(hidden//2)\n",
    "        self.fc3 = nn.Linear(hidden//2, num_classes); self.drop = nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x))); x = self.drop(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x))); x = self.drop(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"# Focal loss with optional class weights α (ENS).\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__(); self.alpha = alpha; self.gamma = gamma\n",
    "    def forward(self, logits, targets):\n",
    "        ce = F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = (1 - pt) ** self.gamma * ce\n",
    "        if self.alpha is not None: loss = self.alpha[targets] * loss\n",
    "        return loss.mean()\n",
    "\n",
    "def effective_number_weights(y, beta=0.999):\n",
    "    \"\"\"# ENS weights for class imbalance.\"\"\"\n",
    "    counts = np.bincount(y, minlength=len(CLASSES))\n",
    "    eff = 1.0 - np.power(beta, counts)\n",
    "    w = (1.0 - beta) / (eff + 1e-8)\n",
    "    w = w / w.sum() * len(CLASSES)\n",
    "    return torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "def train_model(Xtr, ytr, Xva, yva, epochs=30, batch=32, lr=1e-3):\n",
    "    \"\"\"# Train MLP with focal loss; save loss curves.\"\"\"\n",
    "    ds_tr, ds_va = MammogramDS(Xtr, ytr), MammogramDS(Xva, yva)\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch, shuffle=True)\n",
    "    dl_va = DataLoader(ds_va, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = MLP(in_dim=Xtr.shape[1], hidden=256, num_classes=len(CLASSES)).to(device)\n",
    "    alpha = effective_number_weights(ytr).to(device)\n",
    "    crit = FocalLoss(alpha=alpha, gamma=2.0)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
    "\n",
    "    best_state, best_val = None, float(\"inf\")\n",
    "    tr_hist, va_hist = [], []\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train(); tr_loss = 0.0\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(); out = model(xb)\n",
    "            loss = crit(out, yb); loss.backward(); opt.step()\n",
    "            tr_loss += loss.item()\n",
    "        sch.step()\n",
    "\n",
    "        model.eval(); va_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                va_loss += crit(model(xb), yb).item()\n",
    "\n",
    "        tr_loss /= max(1, len(dl_tr)); va_loss /= max(1, len(dl_va))\n",
    "        tr_hist.append(tr_loss); va_hist.append(va_loss)\n",
    "        if va_loss < best_val:\n",
    "            best_val, best_state = va_loss, {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        if (ep+1) % 5 == 0 or ep == 0:\n",
    "            print(f\"[EPOCH {ep+1:>2}/{epochs}] Train {tr_loss:.4f} | Val {va_loss:.4f}\")\n",
    "\n",
    "    # Loss curves → JPG + DOCX\n",
    "    fig = plt.figure(figsize=(7.2, 4.5))\n",
    "    plt.plot(range(1, epochs+1), tr_hist, label=\"Train\")\n",
    "    plt.plot(range(1, epochs+1), va_hist, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training/Validation Loss\")\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "    save_figure_jpg_and_docx(\n",
    "        fig, RESULTS_DIR / \"Figure_Training_Loss.jpg\", RESULTS_DIR / \"Figure_Training_Loss.docx\",\n",
    "        caption=\"Training and validation loss across epochs.\",\n",
    "        description=\"Curves depict optimization progress and generalization gap; the best model is chosen by minimum validation loss.\"\n",
    "    )\n",
    "\n",
    "    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    return model\n",
    "\n",
    "model_t = train_model(X_train_res, y_train_res, X_val, y_val, epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181fa1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- [9] Evaluation helpers ----------\n",
    "def predict_proba(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.tensor(X, dtype=torch.float32).to(device))\n",
    "        return torch.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "def per_class_specificity(y_true, y_pred, n_classes):\n",
    "    spec = []\n",
    "    for c in range(n_classes):\n",
    "        y_pos = (y_true == c).astype(int)\n",
    "        y_hat = (y_pred == c).astype(int)\n",
    "        tn = np.sum((y_pos == 0) & (y_hat == 0))\n",
    "        fp = np.sum((y_pos == 0) & (y_hat == 1))\n",
    "        spec.append(tn / (tn + fp + 1e-12))\n",
    "    return spec\n",
    "\n",
    "def evaluate_split(name, X, y, save_prefix):\n",
    "    \"\"\"# Compute metrics, confusion matrix, ROC/PR; save JPG + DOCX + tables.\"\"\"\n",
    "    probs = predict_proba(model_t, X)\n",
    "    preds = np.argmax(probs, axis=1)\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y, preds),\n",
    "        \"Balanced Accuracy\": balanced_accuracy_score(y, preds),\n",
    "        \"Macro F1\": f1_score(y, preds, average=\"macro\"),\n",
    "        \"Macro Precision\": precision_score(y, preds, average=\"macro\"),\n",
    "        \"Macro Recall\": recall_score(y, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "    y_bin = label_binarize(y, classes=list(range(len(CLASSES))))\n",
    "    rows, aucs = [], []\n",
    "    specs = per_class_specificity(y, preds, len(CLASSES))\n",
    "    for i, cls in enumerate(CLASSES):\n",
    "        prc = precision_score(y, preds, labels=[i], average=None, zero_division=0)[0]\n",
    "        rec = recall_score(y, preds, labels=[i], average=None, zero_division=0)[0]\n",
    "        f1c = f1_score(y, preds, labels=[i], average=None, zero_division=0)[0]\n",
    "        auc_i = roc_auc_score(y_bin[:, i], probs[:, i])\n",
    "        aucs.append(auc_i)\n",
    "        rows.append([cls, f\"{prc:.4f}\", f\"{rec:.4f}\", f\"{f1c:.4f}\", f\"{specs[i]:.4f}\", f\"{auc_i:.4f}\"])\n",
    "    metrics[\"Macro AUC\"] = float(np.mean(aucs))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y, preds)\n",
    "    cm_pct = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12) * 100.0\n",
    "    fig = plt.figure(figsize=(7.2, 6))\n",
    "    sns.heatmap(cm_pct, annot=True, fmt=\".1f\", cmap=\"Blues\",\n",
    "                xticklabels=CLASSES, yticklabels=CLASSES, vmin=0, vmax=100)\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.title(f\"{name} — Normalized Confusion Matrix (%)\"); plt.tight_layout()\n",
    "    save_figure_jpg_and_docx(\n",
    "        fig,\n",
    "        RESULTS_DIR / f\"Figure_{save_prefix}_Confusion_Matrix.jpg\",\n",
    "        RESULTS_DIR / f\"Figure_{save_prefix}_Confusion_Matrix.docx\",\n",
    "        caption=f\"{name} confusion matrix (normalized %).\",\n",
    "        description=\"Each cell shows the percentage of images of a true class predicted as each class (rows sum to 100%).\"\n",
    "    )\n",
    "\n",
    "    # ROC + PR curves\n",
    "    fig = plt.figure(figsize=(13, 5))\n",
    "    # ROC\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i, cls in enumerate(CLASSES):\n",
    "        fpr, tpr, _ = roc_curve(y_bin[:, i], probs[:, i])\n",
    "        plt.plot(fpr, tpr, label=f\"{cls} (AUC={auc(fpr, tpr):.2f})\")\n",
    "    plt.plot([0,1],[0,1],\"k--\"); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"{name} ROC\"); plt.legend()\n",
    "    # PR\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i, cls in enumerate(CLASSES):\n",
    "        prec, rec, _ = precision_recall_curve(y_bin[:, i], probs[:, i])\n",
    "        ap = average_precision_score(y_bin[:, i], probs[:, i])\n",
    "        plt.plot(rec, prec, label=f\"{cls} (AP={ap:.2f})\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"{name} Precision–Recall\"); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    save_figure_jpg_and_docx(\n",
    "        fig,\n",
    "        RESULTS_DIR / f\"Figure_{save_prefix}_ROC_PR.jpg\",\n",
    "        RESULTS_DIR / f\"Figure_{save_prefix}_ROC_PR.docx\",\n",
    "        caption=f\"{name} ROC and Precision–Recall curves.\",\n",
    "        description=\"Per-class one-vs-rest ROC and PR curves derived from calibrated softmax probabilities.\"\n",
    "    )\n",
    "\n",
    "    # Tables\n",
    "    overall_df = pd.DataFrame([{k: f\"{v:.4f}\" for k, v in metrics.items()}])\n",
    "    save_df_to_docx(\n",
    "        overall_df,\n",
    "        RESULTS_DIR / f\"Table_{save_prefix}_Overall_Metrics.docx\",\n",
    "        caption=f\"{name} overall performance metrics.\",\n",
    "        description=\"Aggregated classification metrics across classes for the split.\"\n",
    "    )\n",
    "\n",
    "    perclass_df = pd.DataFrame(rows, columns=[\"Class\",\"Precision\",\"Recall\",\"F1\",\"Specificity\",\"AUC\"])\n",
    "    save_df_to_docx(\n",
    "        perclass_df,\n",
    "        RESULTS_DIR / f\"Table_{save_prefix}_PerClass_Metrics.docx\",\n",
    "        caption=f\"{name} per-class performance metrics.\",\n",
    "        description=\"Precision, recall, F1, specificity, and AUC for each class computed one-vs-rest.\"\n",
    "    )\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"\\n[INFO] Evaluating …\")\n",
    "val_metrics  = evaluate_split(\"Validation\", X_val, y_val, \"Validation\")\n",
    "test_metrics = evaluate_split(\"Test\",       X_test, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb7a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------- [10] Experiment summary (Word) ----------\n",
    "summary = Document()\n",
    "summary.add_paragraph().add_run(\"Experiment Summary\").bold = True\n",
    "summary.add_paragraph(\n",
    "    \"Leak-safe pipeline using CLIP (ViT-B/32) features + MLP with focal loss. \"\n",
    "    \"Train/Val/Test split performed BEFORE any augmentation/resampling. \"\n",
    "    \"SMOTETomek applied to training features only. \"\n",
    ")\n",
    "summary.add_paragraph(f\"Device: {device}\")\n",
    "summary.add_paragraph(f\"Train size (post-resample): {len(X_train_res)}  |  Val: {len(X_val)}  |  Test: {len(X_test)}\")\n",
    "summary.add_paragraph(\n",
    "    f\"Validation — Accuracy {val_metrics['Accuracy']:.4f}, \"\n",
    "    f\"Balanced Accuracy {val_metrics['Balanced Accuracy']:.4f}, \"\n",
    "    f\"Macro F1 {val_metrics['Macro F1']:.4f}, Macro AUC {val_metrics['Macro AUC']:.4f}\"\n",
    ")\n",
    "summary.add_paragraph(\n",
    "    f\"Test — Accuracy {test_metrics['Accuracy']:.4f}, \"\n",
    "    f\"Balanced Accuracy {test_metrics['Balanced Accuracy']:.4f}, \"\n",
    "    f\"Macro F1 {test_metrics['Macro F1']:.4f}, Macro AUC {test_metrics['Macro AUC']:.4f}\"\n",
    ")\n",
    "summary.save(str(RESULTS_DIR / \"Summary_Experiment.docx\"))\n",
    "\n",
    "print(\"\\n[DONE] All JPGs and DOCX files are in:\", RESULTS_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
